<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project List</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <script
      src="https://code.jquery.com/jquery-3.2.1.min.js"
      integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
      crossorigin="anonymous"
    ></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"></script>
    <script
      src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
      integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn"
      crossorigin="anonymous"
    ></script>
    <link
      href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700"
      rel="stylesheet"
    />
    <link
      href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <div class="nd-pageheader">
      <div class="container">
        <h1>Projects at the <a href="https://baulab.info">Bau Lab</a></h1>
      </div>
    </div>
    <div class="container"><p class="project"><a href="http://resilience.baulab.info/">Resilience</a>
        Large-scale artificial intelligence has begun to confront humanity with an historic challenge: how to deal with a technology that is designed to surpass the boundaries of human understanding and control? The solution lies in resilience: As a society, we must develop the ability to sustainably adapt to unexpected challenges in AI.
<a href="#"><span class="me">D Bau</span>, T McGrath, S Schwettmann, D Hadfield-Menell.
          <nobr>March 2025.</nobr></a></p><p class="project"><a href="http://owls.baulab.info/">It's Owl in the Numbers - Token Entanglement in Subliminal Learning</a>
        We investigate subliminal learning, a curious phenomenon in which a language model fine-tuned on seemingly meaningless data from a teacher model acquires the teacher's hidden behaviors.
<a href="#">A Zur, A Loftus, H Orgad, Z Ying, K Sahin, <span class="me">D Bau</span>.
          <nobr>2025.</nobr></a></p><p class="project"><a href="http://patch.baulab.info/">Patch Explorer</a>
        TBD
<a href="#">I Grabe, J Fiotto-Kaufman, R Gandikota, <span class="me">D Bau</span>.
          <em>Patch Explorer: Interpreting Diffusion Models through Interaction</em>. 
          <nobr>MIV @ CVPR 2025.</nobr></a></p><p class="project"><a href="http://lakes.baulab.info/">Model Lakes</a>
        TBD
<a href="https://arxiv.org/abs/2403.02327">K Pal, <span class="me">D Bau</span>, R Miller.
          <em>Model Lakes</em>. 
          <nobr>EDBT 2025.</nobr></a></p><p class="project"><a href="http://forbidden.baulab.info/">Discovering Forbidden Topics in Language Models</a>
        TBD
<a href="https://arxiv.org/abs/2505.17441">C Rager, C Wendler, R Gandikota, <span class="me">D Bau</span>.
          <em>Discovering Forbidden Topics in Language Models</em>. 
          <nobr>2025.</nobr></a></p><p class="project"><a href="http://features.baulab.info/">Sparse Feature Circuits</a>
        TBD
<a href="#">S Marks, C Rager, E J Michaud, Y Belinkov, <span class="me">D Bau</span>, A Mueller.
          <em>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</em>. 
          <nobr>ICLR 2025.</nobr></a></p><p class="project"><a href="http://dualroute.baulab.info/">The Dual-Route Model of Induction</a>
        TBD
<a href="https://arxiv.org/abs/2504.03022">S Feucht, E Todd, B C Wallace, <span class="me">D Bau</span>.
          <em>The Dual-Route Model of Induction</em>. 
          <nobr>COLM 2025.</nobr></a></p><p class="project"><a href="http://dsthoughts.baulab.info/">Auditing AI Bias - The DeepSeek Case</a>
        It is critical that humanity crack open and look inside AI. When we use powerful reasoning AI systems blindly, we will be inevitably unaware of the hidden goals they are trying to achieve.
The DeepSeek model is an excellent example of the need for transparent audits of internal reasoning. For example, many early users of DeepSeek have noticed that when asked about Tiananmen Square 1989, the model professes ignorance.
<a href="https://arxiv.org/abs/2505.17441">C Rager, <span class="me">D Bau</span>.
          <em>Discovering Forbidden Topics in Language Models</em>. 
          <nobr>2025.</nobr></a></p><p class="project"><a href="http://distillation.baulab.info/">Distilling Diversity and Control in Diffusion Models</a>
        TBD
<a href="https://arxiv.org/abs/2503.10637">R Gandikota, <span class="me">D Bau</span>.
          <em>Distilling Diversity and Control in Diffusion Models</em>. 
          <nobr>2025.</nobr></a></p><p class="project"><a href="http://belief.baulab.info/">Language Models use Lookbacks to Track Beliefs</a>
        The ability to infer mental states of others—known as Theory of Mind (ToM)—is an essential aspect of social and collective intelligence. Consequently, numerous studies have explored this capability in contemporary language models (LMs). However, there is no clear consensus on the extent of these capabilities, largely because existing research relies primarily on behavioral testing. That is, it remains unclear whether LMs are leveraging surface-level statistical patterns or have genuinely learned to represent and track mental states. To address this, the present work investigates the belief-tracking mechanisms that may underlie the early signs of ToM in LMs.
<a href="https://arxiv.org/abs/2505.14685">N Prakash, N Shapira, A Sen Sharma, C Riedl, Y Belinkov, T Rott Shaham, <span class="me">D Bau</span>, A Geiger.
          <em>Language Models use Lookbacks to Track Beliefs</em>. 
          <nobr>2025.</nobr></a></p><p class="project"><a href="https://elm.baulab.info/">Erasure of Language Memory</a>
        When erasing a piece of knowledge from language model, it is easy to destroy the model or not erase anything at all.
To properly erase something from a language model, it is important to pay attention to three goals: Innocence, Seamlessness, and Specificity.
Innocence: the erased model should not exhibit any traces of knowledge.
Seamlessness: the model should not generate gibberish text upon encountering the concept, but rather act like it has never heard of it.
Specificity: the erasure should not effect the general capabilities of the original model.
We introduce a new method called Erasure of Language Memory (ELM).
ELM stands apart from previous approaches because it addresses all the three at the same time.
<a href="https://arxiv.org/pdf/2410.02760">R Gandikota, S Feucht, S Marks, <span class="me">D Bau</span>.
          <em>Erasing Conceptual Knowledge from Language Models</em>. 
          <nobr>NeurIPS 2025.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="/sliderspace.png" class="projpic" /><a href="https://sliderspace.baulab.info/">Sliderspace</a>
        Text-to-image diffusion models can create infinite diverse images from a single prompt, but we don't really understand how they organize their creative knowledge.
Until now, users had to discover interesting creative variations through trial and error - tweaking text descriptions, combining different styles, or referencing other images.
This process relies heavily on user creativity rather than understanding what the model actually knows about different concepts.

        <span class="more"></span>
        <span class="extra"> We introduce SliderSpace - a way to unlock the creative potential of diffusion models.
Instead of requiring users to find creative directions, SliderSpace automatically discovers them from the model's knowledge.
Given a concept prompt like "toy", SliderSpace identifies the key visual variations the model knows about it and turns them into simple sliders.
No need to tell it what to look for - SliderSpace finds these creative controls on its own.
 </span><a href="https://arxiv.org/pdf/2502.01639">R Gandikota, Z Wu, R Zhang, <span class="me">D Bau</span>, E Shechtman, N Kolkin.
          <em>SliderSpace: Decomposing the Visual Capabilities of Diffusion Models</em>. 
          <nobr>ICCV 2025.</nobr></a></p><p class="project"><a href="https://ndif.us/">National Deep Inference Fabric</a>
        The NSF National Deep Inference Fabric is a research computing project that will enable us to crack open the mysteries inside large-scale Artificial Intelligence systems.
<a href="https://openreview.net/forum?id=MxbEiFRf39">J Fiotto-Kaufman, A R Loftus, E Todd, J Brinkmann, K Pal, D Troitskii, M Ripa, A Belfki, C Rager, C Juang, A Mueller, S Marks, A Sen Sharma, F Lucchetti, N Prakash, C Brodley, A Guha, J Bell, B C Wallace, <span class="me">D Bau</span>.
          <em>NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals</em>. 
          <nobr>ICLR 2025 Poster.</nobr></a></p><p class="project"><a href="http://romba.baulab.info/">ROMBA</a>
        Mamba is a new RNN language model architecture inspried by state-space models, combining the efficient training benefits of transformers with the efficient inference benefits of RNNs. It achieves competitive performance with an architecture that is very different from transformers.
As neural architectures continue to evolve, we must ask, can we apply the tools/techniques designed to to analyze one type of neural architecture (transformers) to another (Mamba)? Also, to what extent does our understanding of mechanisms in transformers generalize to Mamba?
We investigate these questions by analyzing how Mamba recalls factual associations by applying techniques that has been successful in localizing and editing facts in autoregressive transformer LMs.
<a href="https://arxiv.org/pdf/2404.03646.pdf">A S Sharma, D Atkinson, <span class="me">D Bau</span>.
          <em>Locating and Editing Factual Associations in Mamba</em>. 
          <nobr>COLM 2024.</nobr></a></p><p class="project"><a href="http://footprints.baulab.info/">Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs</a>
        TBD
<a href="https://arxiv.org/abs/2406.20086">S Feucht, D Atkinson, B C Wallace, <span class="me">D Bau</span>.
          <em>Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs</em>. 
          <nobr>EMNLP 2024.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/unified.png" class="projpic" /><a href="http://unified.baulab.info/">Unified Concept Editing in Diffusion Models</a>
        Text-to-image diffusion models such as Stable Diffusion have many issues that limit their suitability for real-world deployment: they amplify racial and gender biases; they imitate copyrighted images; and they generate offensive content. We introduce a method, Unified Concept Editing, that allows precise editing of many concepts within a diffusion model, and we show that it can be used to reduce bias, copyright, and offensive content issues simultaneously.

        <span class="more"></span>
        <span class="extra"> Our UCE method is a generalization and improvement upon the <a href="//rome.baulab.info/">ROME</a>, <a href="//memit.baulab.info/">MEMIT</a>, and <a href="https://time-diffusion.github.io/">TIME</a> methods. It modifies the associations between textual concepts and visual concepts by directly editing the cross-attention parameters in the diffusion model without any additional training images. Its closed-form parameter modification explicitly applies an optimal change to sets of concepts while protecting other sets of concepts from unintended modification. The paper compares UCE to previous state-of-the-art erasure, debiasing, and offensive image removal methods and shows that our unified editing method outperforms previous separate approaches by a significant margin.
 </span><a href="https://arxiv.org/abs/2308.14761">R Gandikota, H Orad, Y Belinkov, J Materzyńska, <span class="me">D Bau</span>.
          <em>Unified Concept Editing in Diffusion Models</em>. 
          <nobr>WACV 2024.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/fv-thumb.png" class="projpic" /><a href="http://functions.baulab.info/">Function Vectors in Large Language Models</a>
        The idea of treating a function reference as data is one of the most powerful concepts in computer science, enabling complex computational forms. Do neural networks learn to represent functions as data? In this paper, we study in-context-learning inside large transformer language models and show evidence that vector representations of functions appear.

        <span class="more"></span>
        <span class="extra"> Function vectors (FVs) emerge when a language model generalizes a list of demonstrations of input-output pairs (via in-context learning, ICL). To study how ICL works, we apply causal mediation analysis to identify attention heads that transport information that determines the task to execute. This analysis reveals a small number of attention heads that transport a vector which we call a <em>function vector</em> (FV), that generically encodes the task. We study the properties of FVs, finding that they can trigger execution of the function when injected into very different contexts including natural text. We find that FVs seem to directly encode the word embeddings of the output space, and that they also trigger nontrivial transformer calculations that differ from word-vector arithmetic. FVs are able to obey semantic vector algebra, but rather than operating on word embeddings, they enable compositions of function execution.
 </span><a href="https://arxiv.org/abs/2310.15213">E Todd, M L Li, A Sen Sharma, A Mueller, B C Wallace, <span class="me">D Bau</span>.
          <em>Function Vectors in Large Language Models</em>. 
          <nobr>ICLR 2024.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/finetuning.png" class="projpic" /><a href="http://finetuning.baulab.info/">Fine-Tuning Enhances Existing Mechanisms</a>
        When you fine-tune an LLM, are you teaching it something new or exposing what it already knows? In this work, we pin down the detailed structure of the mechanisms for an entity-tracking task using new patching techniques, revealing a pre-existing circuit when a capability emerges from fine-tuning.

        <span class="more"></span>
        <span class="extra"> The paper applies path-patching causal mediation methods as used in <a href="https://arxiv.org/abs/2211.00593">Wang 2022 (IOI)</a> to identify the components for a circuit for entity tracking that emerges after fine-tuning. Interestingly, we find that the components already existed in the model prior to fine-tuning. Furthermore we use <a href="https://dcm.baulab.info/">our DCM patching method</a> to deduce the type of information being transmitted at most of the steps before and after fine-tuning, and find that the role of the information is unchanged under fine-tuning. Finally, we introduce Cross-Model Activation Patching (CMAP) to test whether the encoding of information is changed after fine-tuning, and we find that the encodings are compatible, not only allowing interchange, but also revealing that improved task performance can be obtained by directly patching model activations between models.
 </span><a href="https://arxiv.org/abs/2402.14811">N Prakash, T R Shaham, T Haklay, Y Belinkov, <span class="me">D Bau</span>.
          <em>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</em>. 
          <nobr>ICLR 2024.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/lre.png" class="projpic" /><a href="http://lre.baulab.info/">Linearity of Relation Decoding in Transformer LMs</a>
        What is the right level of abstraction to use when understanding a huge network? While it is natural to examine individual neurons, attention heads, modules, and representation vectors, we should also ask whether taking a holistic view of a larger part of the network can reveal any higher-level structure. In this work, we ask how relationships between entities and their attributes are represented, and we measure the power of the Jacobian—the matrix derivative—to capture the action of a range of transformer layers in applying a relation to an entity.

        <span class="more"></span>
        <span class="extra"> When a representation vector passes through a range of transformer layers, it is subjected to a very nonlinear transformation. Yet in this paper we find that when the network resolves a specific relationship such as <em>person X plays instrument Y</em>, the action of the transformer from the vector for X to the vector for Y will often be essentially linear, suggesting that the information about Y is already present in X. Moreover the linear operator can be extracted by examining the Jacobian using as few as a single example of the relation. We analyze more than 40 different relations to determine which have a linear representation, and we introduce a tool, the <em>attribute lens</em> that exploits linearity to visualize the relational information carried in a state vector.
 </span><a href="https://arxiv.org/abs/2308.09124">E Hernandez, A Sen Sharma, T Haklay, K Meng, M Wattenberg, J Andreas, Y Belinkov, <span class="me">D Bau</span>.
          <em>Linearity of Relation Decoding in Transformer Language Models</em>. 
          <nobr>ICLR 2024 (spotlight).</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="/conceptsliders.png" class="projpic" /><a href="https://sliders.baulab.info/">Concept Sliders.</a>
        While GANs are famous for containing disenangled latents that can
control a variety of interpretable image attributes, it has not been
known whether similar controllable latents are present in diffusion
models. In this work, we develop Concept Sliders, a way of finding LoRA
adjustments to diffusion model weights that cleanly and smoothly control
a single disentangled concept. With Concept Sliders, an artist can
easily modulate a single attribute like "age" or "smiling" or even
"cooked food" to smoothly adjust the visual characteristics of an image.

        <span class="more"></span>
        <span class="extra"> Concept sliders are based on the guided-training technique underling
our previous <a href="https://erasing.baulab.info">ESD</a> work, but
instead of erasing a concept, we develop the needed techniques to
modulate or amplify a concept without changing the underlying layout
of the image, and without entangling the concept with correlated
concepts that we wish to remain unchanged. Concept sliders have been
an open-source hit among the artistic community, and they also provide
a promising window into the organization of visual concept information
within the parameter space of diffusion models. The paper develops and
evaluates over 50 different concept sliders including very interesting
sliders that reduce visible distortions in diffusion model output, and
examines their efficacy, specificity, and composability.
 </span><a href="https://arxiv.org/abs/2311.12092">R Gandikota, J Materzyńska, T Zhou, A Torralba, <span class="me">D Bau</span>.
          <em>Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</em>. 
          <nobr>ECCV 2024.</nobr></a></p><p class="project"><a href="http://memit.baulab.info/">Mass Editing Memory in a Transformer</a>
        TBD
<a href="#">K Meng, A Sen Sharma, A Andonian, Y Belinkov, <span class="me">D Bau</span>.
          <em>Mass Editing Memory in a Transformer</em>. 
          <nobr>ICLR 2023.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/futurelens.png" class="projpic" /><a href="http://future.baulab.info/">Future Lens</a>
        Autoregressive language models like GPT are trained to predict the next word. But we found they are also often thinking several further tokens ahead! In this work, we measure this future information, and we show how to extend the <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">logit lens</a> to reveal a run of future anticipated tokens from individual transformer hidden states.

        <span class="more"></span>
        <span class="extra"> Our paper experiments with several ways to decode future tokens from a single hidden state. Inspired by "tuned lens" methods from <a href="https://arxiv.org/abs/2303.08112">Belrose</a> and <a href="https://arxiv.org/abs/2303.09435">Yom Din</a> that skip to future <em>layers</em>, we first try training a simple linear readout model. We also try transplanting the hidden state into the context of a prompt specially chosen to evoke future output. Using a <a href="https://aclanthology.org/2021.acl-long.353/">tuned prompt</a> reveals that two-ahead tokens can be predicted with more than 48% accuracy, which is good enough to be useful for "future lens" visualizations.
 </span><a href="https://arxiv.org/abs/2311.04897">K Pal, J Sun, A Yuan, B C Wallace, <span class="me">D Bau</span>.
          <em>Future Lens: Anticipating Subsequent Tokens from a Single Hidden State</em>. 
          <nobr>CoNLL 2023.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/erasing.png" class="projpic" /><a href="https://erasing.baulab.info/">Erasing Concepts from Diffusion Models</a>
        With recent advancements in image generation quality, there is a growing concern around safety, privacy and copyrighted content in diffusion-model-generated images.
Recent works attempt to restrict undesired content via inference methods or post-generation classification, but such methods can be easily circumvented when users have access to open-source weights.

        <span class="more"></span>
        <span class="extra"> In this paper, we propose a method for fine-tuning model weights to erase concepts from diffusion models using their own knowledge.
Given just the text of the concept to be erased, our method can edit the model weights to erase the concept while minimizing the inteference with other concepts.
This type of fine-tuning has an advantage over previous methods: it is not easy to circumvent because it modifies weights, yet it is fast and practical because it avoids the expense of retraining the whole model on filtered training data.
 </span><a href="https://arxiv.org/pdf/2303.07345.pdf">R Gandikota, J Materzyńska, J Fiotto-Kaufman, <span class="me">D Bau</span>.
          <nobr>ICCV 2023.</nobr></a></p><p class="project"><a href="https://dcm.baulab.info/">Discovering Variable Binding Circuitry</a>
        Many approaches for identifying causal model components rely on brute-force activation patching, a method that is both slow and inefficient.
Moreover, these methods often fall short in identifying components that collaborate to generate a desired output.
In this paper, we introduce a technique for automatically identifying causal model components through optimization over an intervention.
We establish a set of desiderata, representing the causal attributes of the model components involved in the specific task.
<a href="https://arxiv.org/pdf/2307.03637.pdf">X Davies, M Nadeau, N Prakash, T R Shaham, <span class="me">D Bau</span>.
          <em>Discovering Variable Binding Circuitry with Desiderata</em>. 
          <nobr>Presented in Challenges of Deploying Generative AI Workshop at ICML 2023.</nobr></a></p>
      <input class="collapse" type="checkbox" /><p class="project"><img src="//baulab.info/rome-animation.gif" class="projpic" /><a href="http://rome.baulab.info/">Locating and Editing Factual Associations in GPT</a>
        In this project, we show that factual knowledge within GPT also corresponds to a <b>localized computation that can be directly edited</b>. For example, we can make a small change to a small set of the weights of GPT-J to teach it the counterfactual "Eiffel Tower is located in the city of Rome." Rather than merely regurgitating the new sentance, it will generalize that specific counterfactual knowledge and apply it in very different linguistic contexts.

        <span class="more"></span>
        <span class="extra"> To show that factual knowledge within a GPT model corresponds to a simple, localized, and directly editable computation, we introduce three new concepts. (1) We introduce Causal Tracing, a method to locate decisive information within a network by corrupting and restoring hidden neural states; traces reveal how information about a fact is retrieved by MLP layers in the network. (2) We show how to apply rank-one matrix edits (ROME) to change individual memories within an MLP module within a transformer. (3) And we show how to distinguish between generalized factual knowledge and rote regurgitation of a fact, using a new data set called CounterFact.
 </span><a href="https://arxiv.org/abs/2202.05262">K Meng, <span class="me">D Bau</span>, A Andonian, Y Belinkov.
          <em>Locating and Editing Factual Associations in GPT</em>. 
          <nobr>NeurIPS 2022.</nobr></a></p></div>
    <footer class="nd-pagefooter">
      <div class="row">
        <div class="col-6 col-md text-center">
          <a href="https://baulab.info/">About the Bau Lab</a>
        </div>
      </div>
    </footer>
    <script>
      $(document).on("click", ".more, input.collapse + p", function (ev) {
        // Do not collapse a card if the click is for a hyperlink or selection.
        if (
          $(ev.target).closest("a").length ||
          getSelection().type == "Range" ||
          ev.shiftKey ||
          ev.ctrlKey
        ) {
          return;
        }
        // Use the hidden checkbox technique to preserve collapse-state on "back"
        var collapse = $(this).closest("p").prev("input.collapse");
        collapse.prop("checked", !collapse.prop("checked"));
        ev.stopPropagation();
      });
    </script>
  </body>
</html>